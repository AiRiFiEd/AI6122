{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00ce4e8",
   "metadata": {},
   "source": [
    "# Readme:\n",
    "- Please find all cells up to the 2nd last cell\n",
    "- The 2nd last cell consisten of the interactive GUI\n",
    "- Upon choosing the desired options within the interactive GUI, run the last cell to plot chart\n",
    "- chart plotting is separated from the interactive GUI to reduce computational load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d58961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yuanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yuanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yuanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = os.path.dirname(os.getcwd())\n",
    "import sys\n",
    "sys.path.append(directory)\n",
    "import lxml\n",
    "import datetime\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython import display as ICD\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import src.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87735ba",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de45c5c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIRECTORY_DATA = os.path.join(directory, 'data')\n",
    "DIRECTORY_OUTPUT = os.path.join(DIRECTORY_DATA,'01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3555c",
   "metadata": {},
   "source": [
    "# Parsing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed23fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dblp = data.DatasetDBLP(directory_output=DIRECTORY_OUTPUT)\n",
    "filepath = os.path.join(DIRECTORY_DATA, 'dblp.xml')\n",
    "dblp.from_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ba9cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inproceedings = defaultdict(lambda: defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fa5b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_elements = ['inproceedings']\n",
    "for event, element in dblp.data:\n",
    "    if element.tag in all_elements:\n",
    "        conference_name = element.items()[1][1].split('/')[1]\n",
    "        for sub in element:\n",
    "            if sub.tag == 'title':\n",
    "                t = sub.text\n",
    "            if sub.tag == 'year':\n",
    "                d = sub.text\n",
    "        inproceedings[conference_name][d].append(t)\n",
    "        element.clear()\n",
    "        while element.getprevious() is not None:\n",
    "            del element.getparent()[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbf34c",
   "metadata": {},
   "source": [
    "### Generating Dataset for 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de21dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(inproceedings)\n",
    "with open(os.path.join(DIRECTORY_DATA, 'inproceedings.json'), 'w') as f:\n",
    "    json.dump(json_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e07b34",
   "metadata": {},
   "source": [
    "# Inproceedings Count By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f928c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_inproceedings_count_by_year(conference_name: str):\n",
    "    tmp = []\n",
    "    for k, v in inproceedings[conference_name].items():\n",
    "\n",
    "        tmp.append([k, len(v)])\n",
    "    print(tmp)\n",
    "    df = pd.DataFrame(tmp, columns=['year', 'count'])\n",
    "    df.sort_values('year', inplace=True)\n",
    "    fig = df.plot.bar('year', 'count').get_figure()\n",
    "    fig.savefig(os.path.join(DIRECTORY_OUTPUT, conference_name+'_count.png'))\n",
    "    \n",
    "\n",
    "interact(plot_inproceedings_count_by_year, conference_name=sorted([conf for conf in list(inproceedings.keys()) if conf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a2621",
   "metadata": {},
   "source": [
    "# Title Length by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b617cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_title_length_by_year(conference_name: str):\n",
    "    tmp = []\n",
    "    y = []\n",
    "    for k, v in inproceedings[conference_name].items():\n",
    "        lengths = []\n",
    "        for t in v:\n",
    "            if not t is None:\n",
    "                lengths.append(len(t.split(' ')))\n",
    "\n",
    "        tmp.append( {'label': k,'whislo':min(lengths), 'whishi':max(lengths), 'med':statistics.median(lengths), 'q1':np.percentile(lengths, 25), 'q3':np.percentile(lengths, 75)})\n",
    "        y.append(k)\n",
    "    tmp = sorted(tmp, key=lambda x: x['label'])\n",
    "    y = sorted(y)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.bxp(tmp, showfliers=False)\n",
    "    ax.set_xticks(ticks=range(1, len(y)+1),labels=y, rotation=90)\n",
    "    fig.savefig(os.path.join(DIRECTORY_OUTPUT, conference_name+'_title_length.png'))\n",
    "\n",
    "interact(plot_title_length_by_year, conference_name=sorted([conf for conf in list(inproceedings.keys()) if conf]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c17b0a",
   "metadata": {},
   "source": [
    "# Top 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bc0d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TOPN = 5\n",
    "\n",
    "ngrams_radio = widgets.ToggleButtons(\n",
    "    options=['unigram', 'bigram', 'trigram', 'combined'],\n",
    "    description='ngram:',\n",
    "    disabled=False,\n",
    "    tooltips=['top unigrams', 'top bigrams', 'top trigrams', 'top combined'],\n",
    ")\n",
    "\n",
    "stem_radio = widgets.ToggleButtons(\n",
    "    options=['none', 'porter'],\n",
    "    description='stemmer:',\n",
    "    disabled=False,\n",
    "    tooltips=['no stemming', 'stemming using porter stemmer'],\n",
    ")\n",
    "\n",
    "nounonly_checkbox = widgets.ToggleButtons(\n",
    "    options=['all', 'nouns_only'],\n",
    "    description='tokens:',\n",
    "    disabled=False,\n",
    "    tooltips=['use all available tokens', 'use nouns only'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6b12e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking = None\n",
    "totalyears = None\n",
    "def generate_top_10_unigrams_new(ngram: str, nounonly: str, stemmer: str, conference_name: str):\n",
    "    years = sorted([int(y) for y in list(inproceedings[conference_name].keys()) if y])\n",
    "    global totalyears\n",
    "    totalyears = years\n",
    "    if ngram == 'unigram':\n",
    "        n = 1\n",
    "    elif ngram == 'bigram':\n",
    "        n = 2\n",
    "    elif ngram == 'trigram':\n",
    "        n = 3\n",
    "    elif ngram == 'combined':\n",
    "        n = 3\n",
    "        \n",
    "    def get_titles(year: int):\n",
    "        if ngram == 'combined':\n",
    "            outputs = [defaultdict(lambda:defaultdict(int)) for i in range(n)]\n",
    "        else:\n",
    "            outputs = [defaultdict(lambda:defaultdict(int))]\n",
    "        sw = set(stopwords.words(\"english\") + list(string.punctuation))\n",
    "        ps = PorterStemmer()\n",
    "        # counts = defaultdict(lambda:defaultdict(int))\n",
    "        for yr in years:\n",
    "            titles = inproceedings[conference_name][str(yr)]\n",
    "            for t in titles:\n",
    "                if t:\n",
    "                    tokens = word_tokenize(t)\n",
    "                    if nounonly == 'nouns_only':\n",
    "                        tokens = nltk.tag.pos_tag(tokens)\n",
    "                        tokens = [word[0] for word in tokens if (word[1]=='NN')]    \n",
    "                    tokens = [word for word in tokens if (not word.lower() in sw)]\n",
    "                    if ngram == 'combined':\n",
    "                        for i in range(n):\n",
    "                            ngs = ngrams(tokens,i+1)\n",
    "                            for gram in ngs:\n",
    "                                if stemmer != 'none':\n",
    "                                    phrase = ps.stem(TreebankWordDetokenizer().detokenize(gram))\n",
    "                                else:\n",
    "                                    phrase = TreebankWordDetokenizer().detokenize(gram)\n",
    "                                outputs[i][str(yr)][phrase] += 1   \n",
    "                    else:\n",
    "                        ngs = ngrams(tokens,n)\n",
    "                        for gram in ngs:\n",
    "                            if stemmer != 'none':\n",
    "                                phrase = ps.stem(TreebankWordDetokenizer().detokenize(gram))\n",
    "                            else:\n",
    "                                phrase = TreebankWordDetokenizer().detokenize(gram)\n",
    "                            outputs[0][str(yr)][phrase] += 1      \n",
    "        topten = dict()\n",
    "        for output in reversed(outputs): \n",
    "            for k, v in output.items():\n",
    "                _ = [[k,v] for k,v in v.items()]\n",
    "                _ = sorted(_,key=lambda x: x[1],reverse=True)\n",
    "                \n",
    "                if ngram == 'combined':\n",
    "                    top = [t for t in _ if t[1] > 1]\n",
    "                else:\n",
    "                    top = _\n",
    "                if k in topten:\n",
    "                    topten[k] += top\n",
    "                else:\n",
    "                    topten[k] = top\n",
    "        trend = []\n",
    "        for k, v in topten.items():\n",
    "            if len(v)>TOPN:\n",
    "                topten[k] = v[:TOPN]\n",
    "            _ = [phrase[0] for phrase in topten[k]]\n",
    "            _ = [str(k)] + _\n",
    "            trend.append(_)\n",
    "        global ranking \n",
    "        ranking = topten\n",
    "        currenttrend = pd.DataFrame(topten[str(year)], columns=['word', 'count'])\n",
    "        ICD.display(currenttrend)\n",
    "        \n",
    "        trend = pd.DataFrame(trend, columns=['year'] + list(range(1,TOPN+1)))\n",
    "        \n",
    "                \n",
    "        return trend\n",
    "    \n",
    "    interact(get_titles, year=years)    \n",
    "    \n",
    "interact(generate_top_10_unigrams_new, \n",
    "    ngram = ngrams_radio,\n",
    "    nounonly = nounonly_checkbox,\n",
    "    stemmer = stem_radio,\n",
    "    conference_name=sorted([conf for conf in list(inproceedings.keys()) if conf]),\n",
    "    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af52f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bywords = dict()\n",
    "bywordscount = defaultdict(int)\n",
    "\n",
    "for k, v in ranking.items():\n",
    "    for word in v:\n",
    "        bywords[word[0]] = [0] * len(totalyears)\n",
    "\n",
    "counter = 0\n",
    "for k, v in ranking.items():\n",
    "    rank = len(v)\n",
    "    for i in range(len(v)):\n",
    "        \n",
    "        bywords[v[i][0]][counter] = (rank-i)\n",
    "        bywordscount[v[i][0]] += 1\n",
    "    counter += 1\n",
    "    \n",
    "relevantwords = [k for k,v in bywordscount.items() if v > 3]\n",
    "results = {word:bywords[word] for word in relevantwords}\n",
    "results = pd.DataFrame(results, index=totalyears)\n",
    "p = results.plot()\n",
    "fig = p.get_figure()\n",
    "fig.savefig('trend.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bb75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
